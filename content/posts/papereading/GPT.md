---
title: "GPT"
---
**特征**

- self-supervised learning 
- 基于transformer

BERT 与 GPT

​	BERT使用编码器而GPT使用解码器(Transformer)

->GPT2

​	提出zero-shot

```

```

->GPT3

​	用few-shot

​	在做子任务时，不需要微调，不需计算重新更新梯度

---

**Question**：

- zero-shot

- 为什么大模型不容易过拟合？

  (为什么：在大模型中不需要像较小模型一样选择较小的Batch_Size来使每次采样时数据中的噪音增多来避免过拟合)
