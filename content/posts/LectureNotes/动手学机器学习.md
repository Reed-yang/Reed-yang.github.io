---
title: "动手学机器学习"
---
This notes mainly according to [zh.d2l.ai](https://zh.d2l.ai) textbook. Video based notes referred to [here](Rush to Nerf.md).

### 1.前言

 #### 1.2关键组件
  ##### 1.2.1数据
  与传统机器学习方法相比，深度学习的一个主要优势是可以处理不同长度的数据。
  ##### 1.2.2模型
  *深度学习*(deep learning)与经典方法区别在于，关注功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转化。
  ##### 1.2.3目标函数
  *目标函数*
  *损失函数*
  *平方误差*
  *训练集*(training set)*测试集*(test set)
  *过拟合*(overfitting)

  ##### 1.2.4优化算法
  *梯度下降*(gradient descent)
### 2.预备知识 chapter_preliminaries
 #### 2.1数据操作 ndarray
  索引和切片
	**遵循左闭右开原则，如：[0:9]等价于数学中的[0,9)**
	如果我们想[**为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。**] 例如，`[0:2, :]`访问第1行和第2行的全部元素

 #### 2.3线性代数
  *范数*：
  在线性代数中，向量范数是将向量映射到标量的函数$f$。
  给定任意向量$\mathbf{x}$，向量范数要满足一些属性。
  第一个性质是：如果我们按常数因子$lpha$缩放向量的所有元素，
  其范数也会按相同常数因子的*绝对值*缩放：
$$
f(lpha \mathbf{x}) = |lpha| f(\mathbf{x})
$$
  第二个性质是我们熟悉的三角不等式:
$$
  f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y}).
$$
  第三个性质简单地说范数必须是非负的:
$$
  f(\mathbf{x}) \geq 0.
$$
  这是有道理的。因为在大多数情况下，任何东西的最小的*大小*是0。
  最后一个性质要求范数最小为0，当且仅当向量全由0组成。
$$
orall i, [\mathbf{x}]_i = 0 \Leftrightarrow f(\mathbf{x})=0.
$$
  你可能会注意到，范数听起来很像距离的度量。
  如果你还记得欧几里得距离和毕达哥拉斯定理，那么非负性的概念和三角不等式可能会给你一些启发。
  事实上，欧几里得距离是一个$L_2$范数：
  假设$n$维向量$\mathbf{x}$中的元素是$x_1,\ldots,x_n$，其[**$L_2$*范数*是向量元素平方和的平方根：**]
$$
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}
$$
  其中，在$L_2$范数中常常省略下标$2$，也就是说$\|\mathbf{x}\|$等同于$\|\mathbf{x}\|_2$。
  在代码中，我们可以按如下方式计算向量的$L_2$范数。

  ```python
  u = torch.tensor([3.0, -4.0])
  torch.norm(u)
  ```
  另有$L_1$范式，它表示为向量元素的绝对值之和：
$$
\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i ight|
$$
  使用如下代码计算：

  ```python
  torch.abs(u).sum()
  ```



 #### 2.4微积分
  补充：[矩阵求导与分子/分母布局](https://zhuanlan.zhihu.com/p/263777564)

### 3.线性神经网络 chapter_linear-networks

在机器学习的术语中，该数据集称为*训练数据集*（training data set） 或*训练集*（training set）。 每行数据（比如一次房屋交易相对应的数据）称为*样本*（sample）， 也可以称为*数据点*（data point）或*数据样本*（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为*标签*（label）或*目标*（target）。 预测所依据的自变量（面积和房龄）称为*特征*（feature）或*协变量*（covariate）。



(证明略)在高斯噪声的假设下，最小化均方误差等于对线性模型的极大似然估计。



集成学习(ensemble learning)

### 4.多层感知机

训练数据集：训练模型参数

验证数据集：选择模型超参数

测试数据集：用来评估模型和所选参数的泛化能力，但不能作为调参的依据



不够大的数据集上通常使用：K-则交叉验证



从线性到非线性：由于多层仿射变换等价于单次仿射变换，需要在每层仿射变换后应用**激活函数**



过拟合于欠拟合

防止过拟合的trick

​	权重衰退：weight decay，也即正则化，

​	丢弃法：dropout(不用在cnn中，用在mlp上)



深度神经网络可能遇到：梯度爆炸/梯度消失

- 将乘法变加法 Resnet LSTM
- 归一化  梯度归一化 梯度裁剪
- 合理的权重初始和激活函数





Conv：

<img src="https://s2.loli.net/2022/07/16/z9UywcviTZ6Lagr.png" alt="卷积层输出大小" style="zoom: 50%;" />

Pool:

<img src="https://s2.loli.net/2022/07/16/ywCuoVN4gQ7GqRH.png" alt="池化层输出大小" style="zoom:60%;" />
